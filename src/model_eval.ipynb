{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesize information about trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Does dropping correlated features lead to a better model? (no)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                best_cv_acc  test_acc  test_precision  test_recall   test_f1  \\\n",
      "model                                                                          \n",
      "knn                0.003044  0.009091        0.036782     0.018182  0.021261   \n",
      "logreg_elastic    -0.001081  0.000000        0.000000     0.000000  0.000000   \n",
      "logreg_l2          0.003215 -0.018182       -0.035714    -0.054545 -0.056982   \n",
      "rfc               -0.009502  0.009091        0.006667     0.018182  0.017647   \n",
      "svc                0.002162 -0.009091       -0.003333     0.036364  0.003890   \n",
      "xgb                0.001991  0.009091        0.029212    -0.018182 -0.003896   \n",
      "\n",
      "                test_roc_auc  \n",
      "model                         \n",
      "knn             4.462810e-02  \n",
      "logreg_elastic  2.220446e-16  \n",
      "logreg_l2      -1.322314e-02  \n",
      "rfc            -6.611570e-03  \n",
      "svc            -4.958678e-03  \n",
      "xgb             5.785124e-02  \n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "def load_agg(p: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(p)\n",
    "    df[\"metric\"] = df[\"metric\"].astype(str)\n",
    "\n",
    "    return df.set_index(\"metric\")\n",
    "\n",
    "# compare deltas of model accuracy across dfs with correlated cols deleted vs not deleted\n",
    "def compare_agg_folders(no_corr_dir: str, corr_dir: str, out_p: str = \"../results/deltas.csv\"):\n",
    "    no_files = {p.name: p for p in Path(no_corr_dir).glob(\"*_agg.csv\")}\n",
    "    co_files = {p.name: p for p in Path(corr_dir).glob(\"*_agg.csv\")}\n",
    "    \n",
    "    common = sorted(set(no_files) & set(co_files))\n",
    "    metric_cols = [\"best_cv_acc\", \"test_acc\", \"test_precision\", \"test_recall\", \"test_f1\", \"test_roc_auc\"]\n",
    "\n",
    "    wide_rows = {}\n",
    "    for fname in common:\n",
    "        df_no = load_agg(no_files[fname])\n",
    "        df_co = load_agg(co_files[fname])\n",
    "\n",
    "        # delta_mean = no_corr - corr\n",
    "        delta_mean = df_no[\"mean\"] - df_co[\"mean\"]\n",
    "\n",
    "        model = fname.replace(\"_agg.csv\", \"\")\n",
    "        wide_rows[model] = delta_mean.reindex(metric_cols)\n",
    "\n",
    "    deltas = pd.DataFrame.from_dict(wide_rows, orient=\"index\")\n",
    "    deltas.index.name = \"model\"\n",
    "    deltas.to_csv(out_p)\n",
    "\n",
    "    return deltas\n",
    "\n",
    "deltas = compare_agg_folders(no_corr_dir=\"../results/no_ftrs_dropped\", corr_dir=\"../results/ftrs_dropped\",)\n",
    "\n",
    "print(deltas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Find top two models for primary and secondary evaluation metrics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             model          metric      mean       std\n",
      "0   logreg_elastic     best_cv_acc  0.594708  0.025016\n",
      "1              xgb     best_cv_acc  0.564040  0.015839\n",
      "2   logreg_elastic        test_acc  0.681818  0.101639\n",
      "3        logreg_l2        test_acc  0.645455  0.126131\n",
      "4   logreg_elastic         test_f1  0.682120  0.079114\n",
      "5        logreg_l2         test_f1  0.584508  0.192599\n",
      "6   logreg_elastic  test_precision  0.707172  0.132753\n",
      "7        logreg_l2  test_precision  0.705714  0.212756\n",
      "8   logreg_elastic     test_recall  0.672727  0.081312\n",
      "9              rfc     test_recall  0.527273  0.076060\n",
      "10  logreg_elastic    test_roc_auc  0.690909  0.083129\n",
      "11       logreg_l2    test_roc_auc  0.667769  0.180205\n"
     ]
    }
   ],
   "source": [
    "# find best 2 models for each eval metric\n",
    "\n",
    "root = Path(\"../results/no_ftrs_dropped\") \n",
    "\n",
    "rows = []\n",
    "\n",
    "for p in root.rglob(\"*_agg.csv\"):\n",
    "    df = pd.read_csv(p)\n",
    "\n",
    "    model = p.stem.replace(\"_agg\", \"\")\n",
    "\n",
    "    for _, r in df.iterrows():\n",
    "        rows.append({\"model\": model, \"metric\": r[\"metric\"], \"mean\": r[\"mean\"], \"std\": r[\"std\"]})\n",
    "\n",
    "all_metrics = pd.DataFrame(rows)\n",
    "\n",
    "per_metric = (all_metrics\n",
    "    .sort_values([\"metric\", \"mean\", \"std\"], ascending=[True, False, True])\n",
    "    .groupby(\"metric\", as_index=False).head(2).reset_index(drop=True))\n",
    "\n",
    "per_metric.to_csv('../results/best_per_metric.csv', index=False)\n",
    "print(per_metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
